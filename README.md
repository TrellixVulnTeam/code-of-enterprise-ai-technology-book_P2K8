内容简介

《企业级AI技术内幕：深度学习框架开发+机器学习案例实战+Alluxio解密》分为盘古人工智能框架开发专题篇、机器学习案例实战篇、分布式内存管理Alluxio解密篇，分别对人工智能开发框架、机器学习案例及Alluxio系统进行透彻解析。 　　盘古人工智能框架开发专题篇，通过代码讲解多层次神经网络、前向传播算法、反向传播算法、损失度计算及可视化、自适应学习和特征归一化等内容。 　　机器学习案例实战篇，选取机器学习中具代表性的经典案例，透彻讲解机器学习数据预处理、简单线性回归、多元线性回归、多项式回归、支持向量回归、决策树回归、随机森林回归等回归算法，逻辑回归、k近邻算法、支持向量机、朴素贝叶斯、决策树分类、随机森林分类等分类算法、ｋ均值聚类、层次聚类等聚类算法，以及关联分析算法，并对回归模型、分类模型进行性能评估。 　　分布式内存管理Alluxio解密篇，详细讲解Alluxio架构、部署、底层存储及计算应用、基本用法、运行维护等内容。

作者简介

王家林，Apache Spark执牛耳者，现工作于硅谷的AI实验室，专注于NLP框架，超过20本Spark、Al、Android书籍作者，Toastmasters International Division Director ，GRE博士入学考试连续两次满分获得者

段智华，就职于中国电信股份有限公司上海分公司，系统架构师，CSDN博客专家，专注于Spark大数据技术研发及推广，跟随Spark核心源码技术的发展，深入研究Spark 2.1.1版本及Spark 2.2.0版本的源码优化，对Spark大数据处理、机器学习等技术有丰富的实战经验和浓厚兴趣。

图书基本信息

书名： 企业级AI技术内幕：深度学习框架开发+机器学习案例实战+Alluxio解密

出版社： 清华大学出版社

出版日期 2020

ISBN号： 9787302561774

编辑推荐

全代码讲解深度学习框架开发，

案例式讲解机器学习案例，

透彻剖析开源AI和大数据存储编排平台Alluxio

前言/序言

2016年3月，阿尔法狗击败了我们这个星球上最出色的围棋选手，其特殊之处在于阿尔法狗下围棋时使用了一种设计人员没有想到的策略，这是人工智能里程碑式的胜利。

2017年7月，国务院正式发布《新一代人工智能发展规划》，明确把人工智能发展作为国家战略。人工智能是21世纪的三大尖端技术（基因工程、纳米工程、人工智能）之一。国务院发布人工智能分三步走的规划中指出：到2030年，中国的人工智能理论、技术与应用总体达到世界领先水平，成为世界主要人工智能创新中心，智能经济、智能社会取得明显成效，为跻身创新型国家前列和经济强国奠定重要基础。 　　如何驾驭人工智能时代的技术？如何掌握人工智能各种具体的实战技术？如何使用统一内存技术驾驭任意类型的数据？本书基于实践尝试给予这些问题答案。

（1）盘古人工智能框架开发专题篇，包含第1～16章，以图文并茂的形式，带领读者一行一行地编写代码来实现当今主流深度学习框架的核心技术，通过实践解密PyTorch和TensorFlow等最流行技术背后的设计和实现，并讲解深度学习框架在电信运营方面的应用案例。 　

（2）机器学习案例实战篇，包含第17～35章，选取机器学习开发中最具代表性的经典学习案例，透彻讲解机器学习数据预处理，简单线性回归、多元线性回归、多项式回归、支持向量回归、决策树回归、随机森林回归等回归算法，逻辑回归、k近邻算法、支持向量机、朴素贝叶斯算法、决策树分类、随机森林分类等分类算法，k均值聚类、层次聚类等聚类算法，以及关联分析算法，并分别对回归模型、分类模型进行性能评估。 　　

（3）分布式内存管理Alluxio解密篇，包含第36～40章，详细讲解Alluxio架构、部署、底层存储及计算应用、基本用法、运行维护等内容。 　　 基于美国最流行的Heuristic learning（启发式学习）理念，本书所有的内容都是按照具体的问题场景、核心原理、解决方案的顺序组织，均以动手实践的方式一步步驱动学习者流畅地完成，带来无痛苦的学习体验。 　　读者在阅读本书的过程中，如发现任何问题或有任何疑问，可以加入本书的阅读群（QQ：418110145）讨论，会有专人答疑。同时，该群也会提供本书所用案例源代码及本书的配套学习视频。 笔者的新浪微博是http：//weibo.com/ilovepains/，欢迎大家在微博上与我交流。 　　

王家林 2020年 8月

新书案例

案例一：自研盘古人工智能框架

自研盘古人工智能框架案例带领大家开发出自己的深度学习框架，一行一行的写代码，一行一行的测试，自己实现人工智能深度学习的框架。

自研盘古人工智能框架案例实现深度学习神经网络的核心算法功能：多层次神经网络的实现、前向传播算法、反向传播算法、损失度计算及性能调优。我们自己写的深度学习框架的内核机制原理和TensorFlow、PyTorch是一致的。

自研盘古人工智能框架多层次神经网络的实现：实现神经网络的节点结构、实现神经网络层之间节点的连接、如何初始化神经网络的权重及人工智能论文阅读经验分享、实现多个隐藏层。

自研盘古人工智能框架中编码实现前向传播功能：在每个节点上增加数据的输入和计算结果、实现前向传播算法、使用sigmoid函数作为激活函数 、前向传播算法测试并分析计算结果。

自研盘古人工智能框架编码实现反向传播算法功能：深度学习是具体是如何学习的、实现反向传播算法、反向传播算法测试并分析计算结果。

自研盘古人工智能框架编码实现损失度的计算：关于损失度的思考，损失度是所有人工智能框架终身的魔咒、编码实现损失度并进行测试、损失度可视化运行结果 。

通过自适应学习和特征归一化优化自研盘古人工智能框架：自研盘古人工智能框架性能测试及问题剖析、使用特征归一化进行性能优化、采用自适应学习进行性能优化编码。

自研盘古人工智能框架大总结：自研盘古人工智能框架性能测试、神经网络实现及和TensorFlow的对比、前向传播算法实现及其和TensorFlow对比、反向传播算法实现及其和TensorFlow对比、损失度计算实现及其TensorFlow对比、人工智能盘古框架源码。

通过自研盘古人工智能框架案例深入浅出、图文并茂的讲解，透彻剖析深度学习神经网络的思想及原理，带领读者轻松入门深度学习。同时，在自研盘古人工智能框架案例的基础上，对代码进行版本迭代，使用矩阵编写人工智能框架实战，通过四种方式进行性能优化，对感知神经元进行彻底解密，讲解神经网络能够完成各种计算模式的根本原因、用神经网络识别手写数字内幕解密、当今和未来人工智能技术的灵魂近似值及因果关系、损失度及梯度下降的设计与实现、关于梯度下降面对上百万个变量的处理方案、随机梯度下降法及在线机器学习思想揭秘、Mnist图片识别详解、从矩阵视角剖析神经网络反向传播的运行过程、人工智能框架神经网络运行过程中四个核心数学公式的本质剖析。

自研盘古人工智能框架案例使读者能深刻理解深度学习神经网络的精髓及本质，进而理解TensorFlow、PyTorch等人工智能框架的运行机制及原理，将深度学习应用于企业实战。

案例二：基于Pytorch的自然语言处理模型(BERT)的应用案例

本案例是通信行业内部软件人才竞赛的一个比赛案例，基于大数据的甩单订单类型自动识别，我们采用基于Pytorch的自然语言处理模型(BERT)来实现。

通信行业内部竞赛分为2个阶段：初赛根据甩单数据，预测分类是生成一张订单还是多张订单；复赛在预测订单为单订单或多订单的基础上，进一步预测生成订单的具体数量、行项目数量和业务类型。

传统的机器学习文本分类算法首先提取文本的特征（Bag、TF-IDF等），然后将抽取的文本特征输入模型进行训练，常用的机器学习分类器模型包括Bag Naive Bayes、TF-IDF Naive Bayes、TF-IDF SVM、TF-IDF LGB、TF-IDF XGB、Model Ensemble等算法；深度学习算法在文本分类的应用，包括CNN等模型。我们体验了基于Pytorch版本的BERT模型，在文本分类预测时取得了较不错的得分。本案例在通信行业内部竞赛中荣获鼓励奖。

BERT(Bidirectional Encoder Representations from Transformers)是Google 人工智能团队2018年底开源的NLP (Natural Language Processing)自然语言处理模型，Jacob Devlin, Ming-Wei Chang, Kenton Lee和Kristina Toutanova发布论文《Pre-training of Deep Bidirectional Transformers for Language Understanding》。

我们深入研究了Harvard NLP实验室Annotated-Transformer算法源码，Harvard NLP对BERT模型的多注意力模型进行了注释版本的代码复现，让我们对BERT的注意力模型机制有了深入的理解。注意力模型是一个标准编码器-解码器结构，每层有两个子层：第一层是多头自注意机制，第二层是一个简单的、位置导向的、全连接的前馈网络。Decoder 解码器也由一个N=6个相同层的堆栈组成，每层由自注意力、源注意力和前馈网络组成，即在每个编码器层中的两个子层外，解码器还额外插入第三个子层，该子层在编码器堆栈的输出上执行多头注意力关注，解码器与编码器类似，使用残差连接解码器的每个子层，然后进行层归一化。

本案例基于通信行业竞赛官方提供的三个数据文件：订单数据-训练数据、甩单数据-训练数据、甩单数据（测试样本集)，详细讲解了BERT模型在深度学习应用的每一个步骤，包括：收集数据、数据预处理、选择模型、模型训练、模型测试、参数调优等内容，将带领读者对自然语言处理BERT模型进行探索体验。

案例三：人力资源主管正确评估新招聘员工薪水的案例

在机器学习案例实战篇中，选取机器学习开发中最具有代表的经典学习案例，透彻讲解机器学习数据预处理、简单线性回归、多元线性回归、多项式回归、支持向量回归、决策树回归、随机森林回归等回归算法，逻辑回归、K最近邻算法、支持向量机、朴素贝叶斯、决策树分类、随机森林分类等分类算法，K-均值聚类、层次聚类、关联分析算法，并分别对回归模型、分类模型进行性能评估。

本书详细讲解了机器学习算法模型在各案例的应用，例如：工作年限与工资之间的关系案例、人力资源主管正确评估新招聘员工薪水的案例、风投机构评估创业公司的案例、社交网络上进行汽车销售推荐的案例、商场会员信息聚类的案例、关联分析算法案例等。

在人力资源主管正确评估新招聘员工薪水的案例中，实现的核心功能是从机器学习应用算法的角度，根据上一家公司提供的工资级别，人力资源主管预测出应聘者的薪水。例如：如果级别P6是15万美金，级别P7是20万美金，如果应聘者称薪水在P6和P7之间，应聘者可能在P6级别已经工作2年了，应聘者提出要16.5万美金，人力资源主管如何评估工资？从算法的角度，P6与P7之间是一个区间范围，我们利用一根平滑的曲线连接P6与P7，从P6到P7可能需要5年的时间，如果应聘者在P6与P7之间，16.5万美金是合理的，也可能是17万美金，或者是17.5万美金，这是机器学习算法的预测。

人力资源主管正确评估新招聘员工薪水的案例中，各机器学习算法模型的比较：

(1) 简单线性回归算法。P6到P7之间的应聘者，线性回归算法的预测工资在35万左右；而对于CEO，CEO的工资本来是100万美金，预测工资只能给65万美金，这个预测结果比较离谱。

(2) 多项式回归算法。采用多项式回归算法，对于CEO给出了一个很合理的工资，工资为100万美金。对于P6到P7之间的应聘者，预测工资在16.5万美金左右，是比较合理的。

(3) 支持向量回归算法。支持向量回归算法对于P6.5级别预测的工资是25.27万美金，由于支持向量回归算法有分类的功能，从实质上讲，CEO和其他的级别不是一个常规的薪水上的关系，因此，发现CEO与员工的工资状态都不一样，这是可以理解的。

(4) 决策树回归算法。决策树回归算法对P6.5级别的预测工资为15万美金，应聘者提出不低于16.5万美金，这也是合理的。决策树在级别P6和级别P7之间求了一个平均值，对特征进行了不同类别的分类，计算很友好。

(5) 随机森林回归算法。随机森林会用每一棵决策树进行预测，然后求每棵决策树预测值的平均值，15.83万年薪这个预测结果比较接近实际情况，因为应聘者的要求是不能低于16.5万美金，可能实际工资情况是16万美金，换工作加了5000美金。

通过人力资源主管正确评估新招聘员工薪水的案例，读者能简单、直观的比较机器学习算法模型的差异，从算法的角度，理解机器学习算法模型对于员工薪水的预测。

案例四： 基于Alluxio+Pytorch的深度学习案例

Alluxio是基于内存的虚拟分布式存储系统，Alluxio项目起源于美国加州大学伯克利分校AMPLab实验室，Alluxio统一了数据访问方式、为上层计算框架和底层存储系统构建了桥梁，应用程序只需要连接Alluxio访问存储在任何底层存储系统中的数据。本书详细讲解Alluxio架构、部署、底层存储及计算应用、基本用法、运行维护及商业案例等内容。

基于Alluxio+Pytorch的深度学习案例中，要实现核心功能是将深度学习框架Pytorch与Alluxio结合起来，Alluxio作为中间统一数据层，Alluxio加载底层存储系统的数据，为上层的深度学习框架层Pytorch提供Alluxio Fuse接口加载数据。案例使用两个LSTM单元学习从不同相位开始的正弦波时序信号，通过深度学习LSTM模型预测未来时序的信号。

(1) 训练数据预先挂载到Alluxio系统。Alluxio提供了统一命名空间系统，本地文件、 云平台、HDFS等文件系统度可以挂载到Alluxio中。

(2) 深度学习框架通过Alluxio Fuse获取本地文件。对于深度学习应用开发者，Alluxio 的底层挂载是透明的，深度学习应用只需与Alluxio交互，就可以从任何数据源获取训练数据，获取云存储（AWS S3，Azure Blob Store，Google云）数据文件的方式，跟获取本地文件及目录的方式一样，简化了数据的获取方式。

(3) Alluxio将网络远程数据在本地进行缓存。Alluxio提供常用数据的本地缓存，因此不需要通过网络IO访问数据，降低深度学习训练成本，加快训练时间。

随着人工智能技术的发展，深度学习及增强学习在各领域广泛应用，在数据访问及数据存储时可能面临一些问题及挑战：

 训练数据无法直接加载。数据分布于各存储系统（HDFS、本地文件、云平台等），深度学习框架（Tensorflow、Pytorch）可以直接加载一些存储系统的文件，但并不能直接获取所有存储系统中的数据，需进行数据的预处理和转换，导致数据加载效率较低。

 新型储存系统数据获取困难。云存储（AWS S3，Azure Blob Store，Google云）及分布式文件系统(HDFDS、ceph)应用越来越广泛，对于每一个新型的存储方式，用户都需深入学习储存系统的配置方式及使用方法，了解存储系统的基本原理及实现机制，在深度学习应用中对于每个存储系统配置相应的适配接口，使得数据获取困难。

 远程存储数据网络开销高及耗时。在分布式云平台系统，分布式计算平台与分布式存储平台可能位于不同的位置集群，深度学习框架通获取远程存储系统数据，会面临网络IO带宽的瓶颈及增加处理远程数据的时间。

对于以上问题，通过Alluxio系统可以帮助解决深度学习的数据访问问题。

案例五：Spark+AI实战案例

Spark+AI实战案例全新阐述大数据在人工智能领域的应用内容，包括深度学习动手实践：人工智能下的深度学习、深度学习数据预处理、单节点深度学习训练、分布式深度学习训练；Spark+PyTorch案例实战：PyTorch在Spark上的安装、使用PyTorch实战图像识别、PyTorch性能调优最佳实践；Spark+TensorFlow实战：TensorFlow在Spark上的安装、TensorBoard解密、Spark TensorFlow的数据转换；Spark上的深度学习内核解密：使用TensorFlow进行图片的分布式处理、数据模型源码剖析、逻辑节点源码剖析、构建索引源码剖析、深度学习下Spark作业源码剖析、性能调优最佳实践。

Spark+AI案例实战包括：Petastorm 数据预处理案例、Tensorflow TFRecord 格式转换案例、PyTorch+GPU单节点的Mnist数字识别案例、Spark+PyTorch+HorovodRunner+CNN的Mnist数字分布式训练案例、Spark+Tensorflow+HorovodEstimator的Mnist数字分布式训练案例、Spark+PyTorch+ResNet的鲜花图像数据识别案例、 Spark+TensorFlow+TensorBoard的Mnist数字识别案例、Spark+TensorFlow+Inception3海洋图片识别案例。

在Spark+PyTorch+ResNet的鲜花图像数据识别案例中：基于ResNet-50网络模型，对鲜花图像数据（郁金香、向日葵、玫瑰、蒲公英、菊花）进行分布式图像识别实战。

(1) 准备预训练的模型和鲜花集数据。从torchvision.models加载预训练的ResNet-50模型，将鲜花数据下载到databricks文件系统空间。 Databricks鲜花文件映射到Driver及Worker节点上的文件，为深度学习工作负载提供高性能的I/O。

(2) 在Spark Driver 节点上加载ResNet50预训练模型，并将ResNet50模型的状态及 参数广播到Spark Executor节点。

(3) 通过Pandas UDF进行模型预测。其中预测分类值是一个大小为1000的数组，根据ResNet-50模型预测1000个分类的概率。案例中鲜花的分类实际为5类：郁金香、向日葵、玫瑰、蒲公英、菊花，改写ResNet-50模型代码进行优化，得到5个类别的预测值。

 微调ResNet-50模型卷积神经网络：自定义全连接层，使用预训练的网络ResNet-50模型来初始化自己的网络，而不是随机初始化模型参数，然后微调卷积神经网络的所有层参数。

 将ResNet-50模型卷积神经网络作为固定的图像特征提取器。自定义全连接层，在ResNet-50网络模型中，冻结除最后一层以外的所有网络（设置requires_grad == False，反向传播时不计算梯度），只重新训练最后的全连接层参数（只有这层参数会在反向传播时更新参数）。

Spark+AI案例基于Databricks云平台，读者按照本书的内容，可以轻松构建、训练和部署深度学习应用程序，可以轻松地使用深度学习的框架，如Tensorflow、Pytorch等，可以在强大的GPU硬件上大规模运行，支持多种编程语言，支持实时数据集的深度学习模型训练。

《Spark大数据商业实战三部曲》第二版购书链接 https://item.jd.com/12744059.html